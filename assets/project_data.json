[
  {
    "image": "images/project_big_data.jpg",
    "title": "Big Data Pipeline",
    "description": "As part of a major Big Data course project in my 7th semester, I and my team developed an end-to-end big data processing pipeline, from data collection to generating real-time visualized insights. This project focused on the financial and investment domain, specifically analyzing the correlation between search trends and the price dynamics of specific cryptocurrencies.\n\nIn this project, data was collected from two primary sources: the Coinranking API for cryptocurrency price data and the Google Trends API for search trend data. To build this pipeline, we extensively utilized the Azure ecosystem due to its comprehensive technology stack and access to free student credits. However, we also integrated other products such as Power BI for data visualization and MongoDB as one of the data storage layers.\n\nThe pipeline was designed to efficiently process data, employing both stream processing and batch processing approaches, ensuring data is always up-to-date. After the ingestion and storage phases, the data proceeds to the preparation and computation stage, leveraging environments like Databricks Workspace for large-scale processing. The final output of this pipeline is real-time data visualization in the form of interactive graphs. These graphs allow users to observe if there is a correlation between the search trends of a particular coin and its price movements. Furthermore, with its real-time capability, users can analyze coin prices at specific times based on comprehensive historical data. This project not only honed my understanding of big data architecture but also my skills in integrating various technologies to produce practical, data-driven solutions that provide added value to users.",
    "button_key": "big_data",
    "tech_stack": ["Azure Function", "Azure SQL", "Databricks", "PowerBI"],
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_car_detect.png",
    "title": "CV Implementation",
    "description": "This project involved developing a deep learning model capable of detecting the open or closed status of car doors and the car hood. The primary objective was to accurately identify the condition of five specific objects: the front left door, front right door, rear left door, rear right door, and the car hood, each with two possible states (open or closed). This model needed to perform effectively despite variations in viewing angles or car positions.\n\nThe development process followed the structured CRISP-DM framework, ensuring a systematic approach from understanding the problem to model deployment. Data collection was a crucial step, involving taking screenshots of various door and hood conditions from different angles and zoom levels using Python from a provided simulation website. An automated Python script was then used for image labeling, assigning a '1' for open and '0' for closed for each object. This process successfully generated a dataset of 960 images capturing diverse door conditions and angles.\n\nFor the modeling phase, we implemented a multi-label classification approach, selecting ResNet50 as our deep learning model due to its robust capabilities in this domain. The model was trained on Google Colab using the collected image dataset. We initially trained the model for 10 epochs, observing a loss function value of approximately 57%. To enhance performance, we retrained the model for 50 epochs, which significantly improved the accuracy to 99% with a reduced loss function value of around 0.1960. The Binary Cross Entropy Loss was used as the loss function and Adam as the optimizer.\n\nThe trained model was then integrated into a simple detection program built with Python, utilizing its GUI features to mirror the car control simulation website. This program continuously screenshotted the 3D car asset from the simulation and fed it to the model for real-time detection. The detection results were then displayed on the GUI, indicating the predicted status of each door and the hood. While the model performed well across various angles, some minor inaccuracies were noted, indicating room for further iteration and optimization. This project provided valuable insights into applying deep learning for object detection in images and emphasized the importance of proper data collection and augmentation techniques for model performance improvement.",
    "button_key": "cv",
    "tech_stack": ["Python", "OpenCV", "Pytorch"],
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_sales_dash.png",
    "title": "Real-time Dashboard",
    "description": "During my internship at MCT, a Telkomsel Authorized Partner, I developed a real-time sales performance dashboard designed to provide critical insights into outlet and item-level sales data. The primary objective of this project was to automate data processing and visualization, transforming raw sales figures into actionable business intelligence.\n\nThe solution involved creating a robust data pipeline. Raw sales data, initially stored in Excel files, was processed and accessed using Python. This processing prepared the data for injection into Google Sheets through the Google Sheets API. By leveraging this API, data was seamlessly transferred, allowing for dynamic manipulation and visualization directly within Google Sheets. The dashboard itself was then built within Google Sheets, utilizing its charting capabilities to present key performance indicators (KPIs) and trends in an easily digestible graphical format.\n\nThis project significantly enhanced the organization's ability to monitor sales performance in near real-time, facilitating quicker decision-making and strategic planning based on current data. It demonstrates my skills in data automation, API integration, data processing with Python, and dashboard creation for business intelligence.",
    "button_key": "realdash",
    "tech_stack": ["Spreadsheet", "Python"],
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_dash_sps.png",
    "title": "Interactive Dashboard",
    "description": "As the Practicum Coordinator for the Data Warehouse Business Intelligence course, one of my key responsibilities included overseeing student practicum assessments, monitoring the grading process by the assistant team, and coordinating closely with the course lecturer. To streamline these crucial tasks and provide rapid insights, I developed an interactive dashboard using spreadsheets.\n\nThis dashboard was specifically designed to offer a quick and comprehensive overview of practicum performance data for lecturers and assistant practicum. Its core functionality enables users to perform dynamic filtering based on class and assessment components, allowing the charts to automatically update and display relevant data. This feature significantly enhances the ability of stakeholders to rapidly analyze student performance trends and monitor grading progress.\n\nThis project showcases my ability to translate complex data into intuitive visual insights, develop interactive data tools, and leverage spreadsheet capabilities for effective data monitoring and coordination in an educational setting. It highlights my practical skills in data visualization and reporting, crucial for informed decision-making.",
    "button_key": "website1",
    "tech_stack": ["Spreadsheet"],
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_helmet.png",
    "title": "Helmet Pose Estimation",
    "description": "This research project addresses a critical road safety issue by proposing a novel computer vision-based system to detect visual distraction in motorcycle riders. Unlike car drivers, motorcyclists are often overlooked in rider monitoring systems, despite facing disproportionately high risks of severe injury or fatality in traffic accidents. Visual distraction, where a rider's gaze is diverted from the road for even a few seconds, can double the risk of a crash.\n\nThe proposed system leverages a two-stage deep learning pipeline. First, it utilizes a YOLO model for accurate helmet detection, localizing the rider's helmet within the image frame. Second, the cropped helmet image is fed into a pose estimation model based on a Convolutional Neural Network (CNN) with a ResNet backbone to predict its 3D orientation (yaw, pitch, and roll). A key contribution of this work is the introduction of a novel dataset for helmet pose estimation, where accurate ground-truth labels are generated via sensor fusion of IMU (MPU6050) and magnetometer (QMC5883L) data, addressing the lack of publicly available specialized datasets. This dataset includes various helmet types, visor configurations, and lighting conditions across an angular range of ±90 degrees for yaw and ±50 degrees for both pitch and roll. The system identifies a distraction event when the helmet's yaw, pitch, or roll angle exceeds a threshold of ±18.6 degrees, and this deviation is maintained continuously for more than 2 seconds. This threshold is adopted from established automotive Driver Monitoring Systems. \n\nThrough extensive experimentation, a classic trade-off between accuracy and efficiency was observed. While a ResNet50 backbone achieved the highest accuracy with the lowest Mean Absolute Error (MAE) for yaw (1.27∘), pitch (0.95 ∘), and roll (0.90∘), the ResNet18 model was ultimately selected as the optimal choice for real-time applications. ResNet18 demonstrated highly competitive accuracy (MAE of 1.47 for yaw, 1.13 for pitch, and 1.06 for roll) while boasting a significantly superior inference time of just 4.117 ms, corresponding to 19.28 FPS. This efficiency is paramount for delivering timely warnings and improving motorcycle safety.\n\nThis project demonstrates strong capabilities in deep learning model design and training, custom dataset creation, sensor data fusion, and the development of intelligent computer vision systems for real-world safety applications.",
    "button_key": "website2",
    "tech_stack": ["Python", "OpenCV", "Pytorch", "Streamlit"],
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_rag_bps.png",
    "title": "RAG Semantic Search",
    "description": "This project involved developing a sophisticated Question-Answering (QA) system utilizing Retrieval-Augmented Generation (RAG) principles to provide comprehensive insights from BPS (Badan Pusat Statistik) data. The project explored and compared two distinct RAG architectures: Semantic Similarity-based RAG and Ontology/Knowledge Graph-based RAG.\n\nThe system's backend was built using FastAPI in Python 3, offering robust API endpoints, while the front-end used plain HTML and JavaScript for a lightweight, interactive user interface. Google Gemini models (gemini-1.5-flash and gemini-1.5-pro) served as the generative AI component.\n\nKey aspects include:\n* Semantic Similarity-based RAG: This approach converted BPS text data into vector embeddings using a pre-trained Sentence Transformer model (all-MiniLM-L6-v2), indexed in a FAISS vector database for efficient semantic search. It excels in flexibility with unstructured data and broad coverage.\n* Ontology/Knowledge Graph-based RAG: This advanced method focused on structured knowledge by defining an ontology of entities and relationships, constructing a knowledge graph using networkx.DiGraph. This significantly enhances precision, factual accuracy, and explainability, crucial for complex relational queries on official statistical data.\n\nThe development process was greatly aided by VSCode GitHub Copilot Agent Mode, which operated in line with the Model Context Protocol (MCP) by intelligently providing context-aware suggestions for code generation, refactoring, and debugging. This project demonstrates a comprehensive understanding of RAG architectures, their implementation for diverse data types, and the integration of modern AI development workflows.",
    "button_key": "website3",
    "tech_stack": ["Python", "Github Copilot", "Flask"],
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_word.png",
    "title": "Public Sentiment Analysis",
    "description": "This project, undertaken for a Data Mining course, delves into public discourse surrounding a prominent topic: the discussion of Indonesia's demographic bonus by the Indonesian Vice President in various online content. The objective was to understand the contextual meaning and relationships between words used in public comments across different social media platforms where this content was shared.\n\nThe process began with web scraping public comments from various platforms using Apify. Subsequently, the collected raw data underwent a rigorous data preprocessing phase. This involved cleaning comments by converting them to lowercase, removing non-alphabetic characters, and tokenizing them. A comprehensive list of Indonesian stopwords, including custom terms relevant to political and demographic discussions, was applied to ensure only meaningful words remained for analysis.\n\nThe core of this project involved training a Word2Vec model on the preprocessed comment data. This model was configured to create 100-dimensional word vectors, capturing contextual relationships between words. Through this, we were able to identify words semantically similar to key terms like 'gibran' and 'demografi', revealing underlying sentiments and associations within the public's discussion.\n\nFinally, the insights were made tangible through visualization. A Word Cloud was generated to visually represent the most frequently occurring and significant terms, providing an immediate grasp of the dominant themes in the discourse. Additionally, other visualizations, such as TSNE plots (though commented out in the provided code, they are a common practice for such projects), could be employed to illustrate the spatial relationships between word vectors, further enhancing understanding of contextual similarities. This project demonstrates proficiency in data acquisition, preprocessing, and the application of word embedding techniques to extract meaningful insights from unstructured text data.",
    "button_key": "wordcloud",
    "tech_stack": ["Python", "Scikit-learn", "Wordcloud"],
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_islah.png",
    "title": "Event Website Develop",
    "description": "This project involved the development of a comprehensive website for ISLAH 2022, an annual event organized by Lembaga Dakwah Kampus (LDK) Al-Fath Telkom University. The primary goal of ISLAH 2022 was to educate and empower Islamic cadres by strengthening fundamental aspects such as knowledge, leadership, and faith. The website served as the central hub for event information, registration, and engagement.\n\nI was responsible for developing the front-end of this website, focusing on creating an intuitive and responsive user experience. The site features a clean and modern design implemented using Bootstrap 5.2.2 along with custom CSS for tailored aesthetics. Key sections of the website include:\n\n* Home/Jumbotron: An engaging landing section introducing the event.\n* About: Detailed information about the purpose and aspirations of ISLAH.\n* Event: A dynamic section featuring an interactive countdown timer to upcoming sessions, highlighting key event dates and encouraging anticipation.\n* Gallery: Showcasing past event highlights, including \"Big Class\" and \"Small Class\" sessions with descriptions of their themes and speakers. Each gallery item provided links to relevant social media posts for further details.\n* Quotes Carousel: A visually appealing section presenting motivational quotes.\n* Contact Form: A functional form for user inquiries, [though not explicitly handled in the provided HTML, it suggests backend integration for message submission].\n* Social Media Integration: Links to the event's Line, Instagram, and Twitter profiles to foster community engagement.\n\nThis project demonstrates my proficiency in front-end web development, including responsive design principles, utilization of CSS frameworks like Bootstrap, and implementation of JavaScript for interactive elements such as countdown timers. It also highlights my ability to structure web content effectively for event promotion and user interaction.",
    "button_key": "islahweb",
    "tech_stack": ["Bootstrap", "HTML", "CSS", "JavaScript"],
    "link": "https://github.com/wafiqj/islahtelu.github.io",
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_gpt.png",
    "title": "GPT API Extension",
    "description": "This project involved the development of 'Quiz with GPT', a Chrome extension designed to streamline information retrieval and enhance user productivity within specific web environments. The core idea behind this extension was to integrate the power of generative AI directly into a user's workflow, specifically for scenarios requiring quick access to contextual information or potential responses, such as during online quizzes or data entry tasks on platforms like the Telkom University Learning Management System (LMS).\n\nThe extension functions by automatically detecting and extracting relevant data—such as quiz questions and multiple-choice options—from the web page. This extracted information is then securely transmitted to the OpenAI API, leveraging models like GPT-3.5 (text-davinci-003) to generate a concise and relevant response. Upon receiving the AI-generated output, the extension intelligently processes it and highlights the corresponding answer directly on the web page using italic styling, minimizing the need for users to switch between multiple applications or browser tabs.\n\nTechnically, this project showcases proficiency in several key areas:\n\n* Chrome Extension Development: Designing and implementing a functional browser extension, including manifest configuration and content script logic.\n* DOM Manipulation: Skillfully extracting dynamic content from complex web pages and programmatically modifying page elements for user feedback.\n* API Integration: Seamlessly connecting to and interacting with external third-party APIs (specifically OpenAI's powerful generative models) to fetch and process data.\n* JavaScript Programming: Developing robust client-side logic to manage data flow, handle asynchronous operations (like API calls), and implement user interface enhancements.\n\nThis project serves as a compelling proof-of-concept for how AI can be integrated directly into browser environments to create more efficient and assistive digital tools, demonstrating an innovative approach to automating data processing and information synthesis in real-time.",
    "button_key": "gptapi",
    "tech_stack": ["GPT API", "Python"],
    "link": "https://github.com/wafiqj/lms-quiz-extension",
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  },
  {
    "image": "images/project_etl.png",
    "title": "ETL Pipeline Development",
    "description": "This project involved the creation of a comprehensive ETL (Extract, Transform, Load) pipeline, specifically designed as a practical exercise for a Data Warehouse Business Intelligence course. The core objective was to simulate a real-world data integration scenario, enabling students to gain hands-on experience in building robust data flows.\n\nThe pipeline was developed using Pentaho Data Integration (Spoon), a widely used open-source ETL tool. It encompasses various stages of data processing, starting from data extraction, followed by multiple transformation steps, and finally loading into a target system. The project also incorporated SQL for database interactions, demonstrating the ability to integrate database operations within the ETL process.\n\nThis work not only facilitated the creation of practical learning material but also showcased my ability to:\n* Design and Implement ETL Workflows: Constructing logical and efficient data pipelines.\n* Utilize ETL Tools: Proficiently using Pentaho Data Integration (Spoon) for data orchestration.\n* Integrate Database Technologies: Applying SQL for data manipulation and interaction with databases.\n* Develop Educational Resources: Creating clear and effective practicum problems that translate theoretical concepts into practical application.\n\nThis project highlights my understanding of data warehousing principles and my technical skills in data integration, essential for building scalable and reliable business intelligence solutions.",
    "button_key": "etldwbi",
    "tech_stack": ["Pentaho", "SQL"],
    "tech_stack_logos": [
      "icons/icons8-spreadsheet-48.png","icons/icons8-spreadsheet-32.png","icons/icons8-spreadsheet.svg"
    ]
  }
]
